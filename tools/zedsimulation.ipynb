{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import time\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from yolox.data.data_augment import preproc\n",
    "from yolox.exp import get_exp\n",
    "from yolox.utils import fuse_model, get_model_info, postprocess\n",
    "from yolox.utils.visualize import plot_tracking\n",
    "from trackers.ocsort_tracker.ocsort import OCSort\n",
    "from trackers.tracking_utils.timer import Timer\n",
    "\n",
    "from threading import Lock, Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
    "\n",
    "from utils.args import make_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zedcamera thread control signal\n",
    "exit_signal = False\n",
    "new_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tread lock intialize\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processing fromo zed api to numpy array type\n",
    "def load_image_into_numpy_array(image):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        image (zedAPI image): image made from zedAPI\n",
    "\n",
    "    Returns:\n",
    "        numpy array: numpy image\n",
    "    \"\"\"\n",
    "    ar = image.get_data()\n",
    "    ar = ar[:, :, 0:3]\n",
    "    (im_height, im_width, channels) = image.get_data().shape\n",
    "    return np.array(ar).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def load_depth_into_numpy_array(depth):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        depth image (zedAPI image): depth image made from zedAPI\n",
    "\n",
    "    Returns:\n",
    "        numpy array: numpy depth image\n",
    "    \"\"\"\n",
    "    ar = depth.get_data()\n",
    "    ar = ar[:, :, 0:4]\n",
    "    (im_height, im_width, channels) = depth.get_data().shape\n",
    "    return np.array(ar).reshape((im_height, im_width, channels)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10134/2656277456.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  depth_np_global = np.zeros([width, height, 4], dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# zed image output size\n",
    "width = 704\n",
    "height = 416\n",
    "confidence = 0.35\n",
    "\n",
    "# make global dummy image variables to contain zed images\n",
    "image_np_global = np.zeros([width, height, 3], dtype=np.uint8)\n",
    "depth_np_global = np.zeros([width, height, 4], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZED image capture thread function (zed cuda 분리를 위한 thread 생성, 안하면 pythorch와 cuda context 차이로 cuda error 생김)\n",
    "def capture_thread_func(svo_filepath=None):\n",
    "    global image_np_global, depth_np_global, exit_signal, new_data, point_cloud\n",
    "    get_cloud = True\n",
    "    zed = sl.Camera()\n",
    "\n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    input_type = sl.InputType()\n",
    "    if svo_filepath is not None:\n",
    "        input_type.set_from_svo_file(svo_filepath)\n",
    "\n",
    "    init = sl.InitParameters(input_t=input_type)\n",
    "    init.camera_resolution = sl.RESOLUTION.HD720\n",
    "    init.camera_fps = 30\n",
    "    init.depth_mode = sl.DEPTH_MODE.PERFORMANCE\n",
    "    init.coordinate_units = sl.UNIT.MILLIMETER\n",
    "    init.svo_real_time_mode = False\n",
    "\n",
    "\n",
    "    # Open the camera\n",
    "    err = zed.open(init)\n",
    "    while err != sl.ERROR_CODE.SUCCESS:\n",
    "        err = zed.open(init)\n",
    "        print(err)\n",
    "        exit(1)\n",
    "    \"\"\"\n",
    "    image_mat = sl.Mat()\n",
    "    depth_mat = sl.Mat()\n",
    "    \"\"\"\n",
    "    runtime = sl.RuntimeParameters()\n",
    "    runtime.sensing_mode = sl.SENSING_MODE.STANDARD\n",
    "    \n",
    "    image_size = zed.get_camera_information().camera_resolution\n",
    "    image_size.width = image_size.width /2\n",
    "    image_size.height = image_size.height /2\n",
    "    \n",
    "    image_zed = sl.Mat(image_size.width, image_size.height, sl.MAT_TYPE.U8_C4)\n",
    "    depth_image_zed = sl.Mat(image_size.width, image_size.height, sl.MAT_TYPE.U8_C4)\n",
    "    point_cloud = sl.Mat()\n",
    "    \n",
    "\n",
    "    while not exit_signal:\n",
    "        if zed.grab(runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "            zed.retrieve_image(image_zed, sl.VIEW.LEFT, resolution=image_size)\n",
    "            zed.retrieve_measure(depth_image_zed, sl.MEASURE.XYZRGBA, resolution=image_size)\n",
    "            lock.acquire()\n",
    "            image_np_global = load_image_into_numpy_array(image_zed)\n",
    "            depth_np_global = load_depth_into_numpy_array(depth_image_zed)\n",
    "            if get_cloud:\n",
    "                zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, sl.MEM.CPU, image_size)\n",
    "            new_data = True\n",
    "            lock.release()\n",
    "\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    zed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image 파일이 데이터 셋일경우 리스트로 만들어주는 함수\n",
    "def get_image_list(path):\n",
    "    image_names = []\n",
    "    for maindir, subdir, file_name_list in os.walk(path):\n",
    "        for filename in file_name_list:\n",
    "            apath = osp.join(maindir, filename)\n",
    "            ext = osp.splitext(apath)[1]\n",
    "            if ext in IMAGE_EXT:\n",
    "                image_names.append(apath)\n",
    "    return image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        exp,\n",
    "        trt_file=None,\n",
    "        decoder=None,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        fp16=False\n",
    "    ):\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        self.num_classes = exp.num_classes\n",
    "        self.confthre = exp.test_conf\n",
    "        self.nmsthre = exp.nmsthre\n",
    "        self.test_size = exp.test_size\n",
    "        #self.test_size = (1920, 1080) 테스트 파일 이미지 크기는 yolox/data/data_augment.py의 preproc 함수를 통해서 만들어짐\n",
    "        self.device = device\n",
    "        self.fp16 = fp16\n",
    "        \n",
    "        if trt_file is not None:\n",
    "            from torch2trt import TRTModule\n",
    "        \n",
    "            model_trt = TRTModule()\n",
    "            model_trt.load_state_dict(torch.load(trt_file))\n",
    "            self.model = model_trt\n",
    "        \n",
    "        else:\n",
    "            self.model = exp.get_model().to(args.device)\n",
    "            \n",
    "        self.rgb_means = (0.485, 0.456, 0.406)\n",
    "        self.std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    def inference(self, img, timer):\n",
    "        img_info = {\"id\": 0}\n",
    "        if isinstance(img, str):\n",
    "            img_info[\"file_name\"] = osp.basename(img)\n",
    "            img = cv2.imread(img)\n",
    "        else:\n",
    "            img_info[\"file_name\"] = None\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "        img_info[\"height\"] = height\n",
    "        img_info[\"width\"] = width\n",
    "        img_info[\"raw_img\"] = img\n",
    "\n",
    "        img, ratio = preproc(img, self.test_size, self.rgb_means, self.std)\n",
    "        img_info[\"ratio\"] = ratio\n",
    "        \n",
    "        img = torch.from_numpy(img).unsqueeze(0).float().to(self.device)\n",
    "        if self.fp16:\n",
    "            img = img.half()  # to FP16\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            timer.tic()\n",
    "            outputs = self.model(img)\n",
    "            if self.decoder is not None:\n",
    "                #pdb.set_trace()\n",
    "                outputs = self.decoder(outputs, dtype=outputs.type())\n",
    "                \n",
    "            outputs = postprocess(\n",
    "                outputs, self.num_classes, self.confthre, self.nmsthre\n",
    "            )\n",
    "        return outputs, img_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthflow_demo(predictor, vis_folder, current_time, args):\n",
    "    global image_np_global, depth_np_global, new_data, exit_signal, point_cloud\n",
    "    # 타임스텝 및 세이브 폴더\n",
    "    timestamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "    save_folder = osp.join(vis_folder, timestamp)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    tracker = OCSort(det_thresh=args.track_thresh, iou_threshold=args.iou_thresh, use_byte=args.use_byte)\n",
    "    timer = Timer()\n",
    "    frame_id = 0\n",
    "    results = []\n",
    "    \n",
    "    # Start the capture thread with the ZED input\n",
    "    capture_thread = Thread(target=capture_thread_func)\n",
    "    capture_thread.start()\n",
    "\n",
    "    key = ' '\n",
    "    prevTime = 0\n",
    "    while key != 113 :\n",
    "        # frame 분석 로거\n",
    "        \"\"\"\n",
    "        if frame_id % 20 == 0:\n",
    "            logger.info('Processing frame {} ({:.2f} fps)'.format(frame_id, 1. / max(1e-5, timer.average_time)))\n",
    "        \"\"\"\n",
    "        \n",
    "        if new_data:\n",
    "            lock.acquire()\n",
    "            image_ocv = np.copy(image_np_global)\n",
    "            depth_image_zed = np.copy(depth_np_global)\n",
    "            new_data = False\n",
    "            lock.release()\n",
    "            \n",
    "            outputs, img_info = predictor.inference(image_ocv, timer)\n",
    "            if outputs[0] is not None:\n",
    "                online_targets = tracker.update(outputs[0], [img_info['height'], img_info['width']], exp.test_size)\n",
    "                online_tlwhs = []\n",
    "                online_ids = []\n",
    "                for t in online_targets:\n",
    "                    tlwh = [t[0], t[1], t[2] - t[0], t[3] - t[1]]\n",
    "                    tid = t[4]\n",
    "                    vertical = tlwh[2] / tlwh[3] > args.aspect_ratio_thresh\n",
    "                    if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n",
    "                        online_tlwhs.append(tlwh)\n",
    "                        online_ids.append(tid)\n",
    "                        results.append(\n",
    "                            f\"{frame_id},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},{tlwh[2]:.2f},{tlwh[3]:.2f},1.0,-1,-1,-1\\n\"\n",
    "                        )\n",
    "                # timer.toc()\n",
    "                online_im = plot_tracking(\n",
    "                    img_info['raw_img'], point_cloud, online_tlwhs, online_ids, frame_id=frame_id + 1, fps=fps\n",
    "                )\n",
    "            else:\n",
    "                timer.toc()\n",
    "                online_im = img_info['raw_img']\n",
    "        else:\n",
    "            online_im = image_np_global\n",
    "            #time.sleep(0.01)\n",
    "        curTime = time.time()\n",
    "        fps = int(1./(curTime - prevTime))\n",
    "        prevTime = curTime\n",
    "        cv2.imshow(\"image\", online_im)\n",
    "        key = cv2.waitKey(10)\n",
    "    cv2.destroyAllWindows()\n",
    "    capture_thread.join()\n",
    "    print(\"\\nFINISH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(exp, args):\n",
    "    if not args.expn:\n",
    "        args.expn = exp.exp_name\n",
    "\n",
    "    output_dir = osp.join(exp.output_dir, args.expn)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if args.save_result:\n",
    "        vis_folder = osp.join(output_dir, \"track_vis\")\n",
    "        os.makedirs(vis_folder, exist_ok=True)\n",
    "\n",
    "    if args.trt:\n",
    "        args.device = \"gpu\"\n",
    "    args.device = torch.device(\"cuda\" if args.device == \"gpu\" else \"cpu\")\n",
    "    \n",
    "    if args.trt:\n",
    "        assert not args.fuse, \"TensorRT model is not support model fusing!\"\n",
    "        trt_file = osp.join(output_dir, \"model_trt.pth\")\n",
    "        assert osp.exists(\n",
    "            trt_file\n",
    "        ), \"TensorRT model is not found!\\n Run python3 tools/trt.py first!\"\n",
    "        model.head.decode_in_inference = False\n",
    "        decoder = model.head.decode_outputs\n",
    "        logger.info(\"Using TensorRT to inference\")\n",
    "    else:\n",
    "        trt_file = None\n",
    "        decoder = None\n",
    "\n",
    "    logger.info(\"Args: {}\".format(args))\n",
    "\n",
    "    if args.conf is not None:\n",
    "        exp.test_conf = args.conf\n",
    "    if args.nms is not None:\n",
    "        exp.nmsthre = args.nms\n",
    "    if args.tsize is not None:\n",
    "        exp.test_size = (args.tsize, args.tsize)\n",
    "        \n",
    "    predictor = Predictor(args, exp, trt_file, decoder, args.device, args.fp16)\n",
    "    logger.info(\"Model Summary: {}\".format(get_model_info(predictor.model, exp.test_size)))\n",
    "    model = predictor.model\n",
    "    model.eval()\n",
    "    \n",
    "    \"\"\"\n",
    "    # 여기에 model 초기화를 하니 trt 모델이 돌아갔습니다... 수정필요\n",
    "    if args.trt:\n",
    "        x = torch.ones((1, 3, 800, 1440), device=\"cuda\")\n",
    "        model(x)\n",
    "    # print(\"=======================\")\n",
    "    # print(model.head.hw)\n",
    "    \"\"\"\n",
    "\n",
    "    if not args.trt:\n",
    "        if args.ckpt is None:\n",
    "            ckpt_file = osp.join(output_dir, \"best_ckpt.pth.tar\")\n",
    "        else:\n",
    "            ckpt_file = args.ckpt\n",
    "        logger.info(\"loading checkpoint\")\n",
    "        ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n",
    "        # load the model state dict\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        logger.info(\"loaded checkpoint done.\")\n",
    "\n",
    "        if args.fuse:\n",
    "            logger.info(\"\\tFusing model...\")\n",
    "            model = fuse_model(model)\n",
    "\n",
    "        if args.fp16:\n",
    "            model = model.half()  # to FP16\n",
    "\n",
    "    current_time = time.localtime()\n",
    "    if args.demo_type == \"image\":\n",
    "        image_demo(predictor, vis_folder, current_time, args)\n",
    "    elif args.demo_type == \"video\" or args.demo_type == \"webcam\":\n",
    "        imageflow_demo(predictor, vis_folder, current_time, args)\n",
    "    elif args.demo_type == \"depthcam\":\n",
    "        depthflow_demo(predictor, vis_folder, current_time, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ocsort')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58b5af387b408301e40b17abed398f3fe69be9f57d3886029e3eb73eb8d2220d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
